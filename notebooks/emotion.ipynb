{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "from retinaface import RetinaFace\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using deepface for face detection\n",
    "face = DeepFace.detectFace(img_path = \"images/img-3.webp\", target_size = (224, 224), detector_backend = \"opencv\")\n",
    "cv2.imshow(\"img\", face)\n",
    "keypressed = cv2.waitKey(0)\n",
    "if keypressed == 27:\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using retina face for face detection and extraction (https://github.com/serengil/retinaface)\n",
    "faces = RetinaFace.extract_faces(img_path=\"images/img-3.webp\", align=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step\n",
      "{'emotion': {'angry': 0.8732596001481772, 'disgust': 0.005419291806136887, 'fear': 48.41676046948197, 'happy': 0.045700536970826204, 'sad': 8.113112039155485, 'surprise': 0.09185165947164056, 'neutral': 42.4538935693439}, 'dominant_emotion': 'fear', 'region': {'x': 66, 'y': 422, 'w': 1073, 'h': 1073}}\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "{'emotion': {'angry': 0.5815741368024512, 'disgust': 0.032024639401728144, 'fear': 1.2264638738589135, 'happy': 96.40222148921745, 'sad': 0.4939510610194279, 'surprise': 0.08827722519588174, 'neutral': 1.175494539051597}, 'dominant_emotion': 'happy', 'region': {'x': 0, 'y': 0, 'w': 1743, 'h': 1182}}\n"
     ]
    }
   ],
   "source": [
    "for face in faces:\n",
    "    # detect emotion with deepface\n",
    "    demography = DeepFace.analyze(img_path=face, actions = ['emotion'], enforce_detection=False, detector_backend=\"opencv\")\n",
    "    x = demography[\"region\"][\"x\"]\n",
    "    y = demography[\"region\"][\"y\"]\n",
    "    w = demography[\"region\"][\"w\"]\n",
    "    h = demography[\"region\"][\"h\"]\n",
    "    print(demography)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_emotion(frame):\n",
    "        \"\"\"\n",
    "        Detects emotion\n",
    "        \"\"\"\n",
    "        # use retina face for face detection and extraction (https://github.com/serengil/retinaface)\n",
    "        result = RetinaFace.detect_faces(img_path=frame)\n",
    "        \n",
    "        for result_key in result:\n",
    "            # get the coordinates of the face on the image\n",
    "            (x, y, w, h) = result[result_key][\"facial_area\"]\n",
    "            \n",
    "            # detect emotion\n",
    "            demography = DeepFace.analyze(img_path=frame[y: h, x: w], actions = ['emotion'], enforce_detection=False, detector_backend=\"opencv\")\n",
    "            \n",
    "            # get the dominant emotion\n",
    "            dominant_emotion = demography[\"dominant_emotion\"]\n",
    "            \n",
    "            # draw bounding boxes of frame\n",
    "            cv2.rectangle(frame, (x, y), (w, h), (0, 255, 0), 5, cv2.LINE_AA)\n",
    "            \n",
    "            # write emotion on bounding box\n",
    "            cv2.putText(frame, dominant_emotion, (x, y), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 1, cv2.LINE_AA)\n",
    "            \n",
    "        return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "neutral\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "happy\n"
     ]
    }
   ],
   "source": [
    "img2 = cv2.imread(\"images/img-3.webp\")\n",
    "img2 = detect_emotion(img2)\n",
    "cv2.imshow(\"img\", img2)\n",
    "keypressed = cv2.waitKey(0)\n",
    "if keypressed == 27:\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ddd6b8294e2de1e2019b8318cf600cc48b04c204c77a1cb963de4ea2b7512d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
